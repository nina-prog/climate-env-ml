{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e90be3f-1e56-4de8-91e3-d6b62a5e75a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise worksheet no 2\n",
    "\n",
    "# Linear models, the bias-variance trade-off, and the curse of dimensionality\n",
    "*Machine learning in climate and environmental sciences, winter semester 2024, Jun.-Prof. Peer Nowack, peer.nowack@kit.edu*\n",
    "\n",
    "*Chair for AI in Climate and Environmental Sciences, https://ki-klima.iti.kit.edu*\n",
    "\n",
    "**Learning goals:** In this notebook, you will practice key concepts of machine learning model optimization, including regularization, cross-validation, and hyperparameter tuning. You will also learn more about the linear statistical learning methods called ridge and LASSO regression. \n",
    "\n",
    "Motivated by Lecture 2, part 1 of this worksheet runs you through a highly visual example of regularizing a 2D-polynomial curve fit. Part 2 considers overfitting in high-dimensions by comparing unregularized multiple linear regression with ridge regression, while revisiting the Worksheet 1 case study of historical global warming. Part 3 compares L1 to L2 regularization by contrasting ridge and LASSO regression, this time focusing on temperature changes in the larger Karlsruhe area. You will also visualize the parameters you have learned and attempt to explain your results, highlighting the good interpretability of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf871c5-fade-4e6f-a6c9-611d878d90cf",
   "metadata": {},
   "source": [
    "## Load Python and the \"ML-climate\" kernel\n",
    "\n",
    "As in the previous week: if you are working on your own computer, now select the Tab \"Kernel\" above, and then select from the drop-down menu the entry \"Change Kernel\" and select \"ML-climate\". This option should exist for you if you followed the Anaconda 3 and subsequent installation instructions provided on Ilias. Alternatively, you can run the notebook on Google Colab, see instructions from last week. We will comment in the cells below whenever code changes are required to make this code run on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad32a8-8b46-4713-805d-f1838605edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### On Google Colab uncomment the following lines\n",
    "# !pip install netcdf4\n",
    "# !pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24e46a-0af8-4c9c-912a-6d77b7944d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the required Python packages\n",
    "# note: we have accummulated a few repeatedly used self-defined functions in the file \"MLclimate.py\", which are imported here\n",
    "### On Google Colab: comment out the two lines above and instead uncomment and execute the following:\n",
    "# from google.colab import files\n",
    "# files.upload() ### in the pop-up window \"Choose Files\" the MLclimate.py file from your exercise folder\n",
    "import MLclimate\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "from cartopy.util import add_cyclic_point\n",
    "from cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d125c-c370-4eca-8c4a-7219578d2182",
   "metadata": {},
   "source": [
    "## Part 1 - Regularization in 2D\n",
    "\n",
    "- In this exercise, you pick up the 2D polynomial curve fitting example from Lecture 2, motivated by chapter 1 in Bishop (2006), *Pattern Recognition and Machine Learning*.\n",
    "- You will explore how you can use L2-regularization to reduce the variance in a 9th-order polynomial in order to obtain a better fit to only 10 noisy samples of a sine wave.\n",
    "  \n",
    "First, we create a sine wave and add noise to ten randomly sampled points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8d769-f266-4848-9aef-7ba9b6ca5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = default_rng(21)\n",
    "random_state_2 = default_rng(303)\n",
    "lightgreen = '#00ff00'\n",
    "### create 10 randomly sampled points from a uniform distribution\n",
    "x10 = random_state.uniform(0,1,size=10)\n",
    "### create the input for a smooth sine function as well\n",
    "x1000 = np.linspace(0,1,1000)\n",
    "### create the sine wave data points and add random noise\n",
    "sine_wave = np.sin(np.pi*2*x10)+random_state_2.uniform(-0.25,0.25,10)\n",
    "sine_wave_smooth = np.sin(np.pi*2*x1000)\n",
    "### plot the wave using matplotlib\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(x10,sine_wave,s=50,edgecolor='blue',facecolor='w',marker='o')\n",
    "plt.plot(x1000,sine_wave_smooth,color=lightgreen)\n",
    "plt.xlabel('x',size=20)\n",
    "plt.ylabel('y',size=20)\n",
    "plt.title('10 samples of a sine wave with noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff16dd9-aaa5-498b-830b-ec1a1d62e0c5",
   "metadata": {},
   "source": [
    "As discussed in Lecture 2, we fit polynomials of order $M$ to the 10 samples, i.e.\n",
    "$$\n",
    "y(x,\\mathbf{w}) = w_0 + w_1 x + w_2 xÂ² + ... +  w_M x^M = \\sum_{j=0}^M w_j x^j\n",
    "$$\n",
    "Specifically, we fit $M$ = 0, 1, 3, and 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cef86-0789-47db-b242-a54cc27cb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = np.polyfit(x10,sine_wave,0)\n",
    "p0 = np.poly1d(z0)\n",
    "z1 = np.polyfit(x10,sine_wave,1)\n",
    "p1 = np.poly1d(z1)\n",
    "z3 = np.polyfit(x10,sine_wave,3)\n",
    "p3 = np.poly1d(z3)\n",
    "z9 = np.polyfit(x10,sine_wave,9)\n",
    "p9 = np.poly1d(z9)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(x10,sine_wave,s=50,color='blue',edgecolor='blue',facecolor='w',marker='o')\n",
    "plt.plot(x1000,p0(x1000),color='k',label='$M$ = 0',linewidth=1.5)\n",
    "plt.plot(x1000,p1(x1000),color='r',label='$M$ = 1',linewidth=1.5)\n",
    "plt.plot(x1000,p3(x1000),color='m',label='$M$ = 3',linewidth=1.5)\n",
    "plt.plot(x1000,p9(x1000),color='y',label='$M$ = 9',linewidth=1.5)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('x',size=20)\n",
    "plt.ylabel('y',size=20)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.title('10 samples of a sine wave with noise + polynomial fits of order $M$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1d797-7ffa-4ae9-ab1a-e85b054ed700",
   "metadata": {},
   "source": [
    "### Part 1.1 Addressing overfitting - option 1: more data\n",
    "\n",
    "The polynomials of orders 0 and 1 cannot fit the variance in the data, whereas $M$ = 9 overfits. The 3rd-order polynomial, however, appears to be a relatively good compromise. \n",
    "\n",
    "In reality and working with machine learning, we would still often want to fit very flexible functions, such as neural networks. Here, let's assume that the 9th-order polynomial is a representative of such more flexible functions.\n",
    "\n",
    "One way to address overfitting for these flexible models would be to collect more data to train them on. To illustrate this, let's try fitting the 9th-order polynomial again, but this time using $N$ = 15 and $N$ = 100 samples to fit the function.\n",
    "\n",
    "**Task 1.1** Complete the code below so that the right subfigure shows the 9th-order polynomial fit to $N$ = 100 samples at the already initialized x-values `x100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4c638-2c26-4b98-a809-66c593cf2d6b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcfc1a9311879b8aaba01e1c9138978c",
     "grade": true,
     "grade_id": "visualize_N100",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "random_state = default_rng(21)\n",
    "random_state_2 = default_rng(303)\n",
    "x15 = random_state.uniform(0,1,15)\n",
    "x100 = random_state.uniform(0,1,100)\n",
    "sine_wave_15 = np.sin(np.pi*2*x15)+random_state_2.uniform(-0.25,0.25,15)\n",
    "z9_15 = np.polyfit(x15,sine_wave_15,9)\n",
    "p9_15 = np.poly1d(z9_15)\n",
    "plt.subplot(121)\n",
    "plt.scatter(x15,sine_wave_15,s=50,edgecolor='blue',facecolor='w',marker='o')\n",
    "plt.plot(x1000,p9_15(x1000),color='r',label='$M$ = 9')\n",
    "plt.plot(x1000,sine_wave_smooth,color=lightgreen,label='True function')\n",
    "plt.legend(fontsize=16,loc='upper right')\n",
    "plt.title('$N$ = 15',size=20)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.xlabel('x',size=20)\n",
    "plt.ylabel('y',size=20)\n",
    "plt.subplot(122)\n",
    "### Task 1.1\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e1fcc-4ccf-4ab3-99aa-620d2d341445",
   "metadata": {},
   "source": [
    "### Part 1.2 Addressing overfitting - option 2: regularization\n",
    "\n",
    "\n",
    "From this new graph, it is evident and intuitive that more data helps to better constrain the variance in the 9th-order polynomial. For $N$ = 100, the resulting function would have good generalizable skill across the interval [0,1]. \n",
    "\n",
    "Unfortunately, in most machine learning challenges simply collecting more data is not an option. In addition, in machine learning, we usually aim to learn functions with many predictors (typically dozens to millions) so that we are also subjected to the *curse of dimensionality*, which we discussed in Lecture 2 and which you will also encounter in Parts 2 and 3 below. This means that relative to the number of predictors, the data available will in most cases be sparse.\n",
    "\n",
    "Instead, we need to find alternative ways to constrain the variance in the 9th-order polynomial. Next, you will apply a key concept to achieve just this, called **L2-regularization.**\n",
    "\n",
    "First, we re-formulate the 9-th order polynomial as a linear model. Second, we fit an initially *unregularized* `Ridge()` regression model from `sklearn.linear_model` (i.e. regularization parameter $\\alpha$ = 0) and demonstrate that this gives equivalent results to fitting a 9th-order polynomial using `numpy.polyfit()` above.\n",
    "\n",
    "Again, ridge regression will allow us to carry out a multiple linear regression (MLR) with L2-regularization. However, first we set the regularization parameter $\\alpha$ to 0 to emulate the case of a simple linear 9th-order polynomial fit without regularization (i.e. MLR). You will learn more about ridge regression in Lecture 3 and can find further detail in its `sklearn` [function documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). The regression logic for training and prediction follows the standard `sklearn` sequence of\n",
    "\n",
    "(a) define the regression object\n",
    "\n",
    "(b) `.fit()`\n",
    "\n",
    "(c) `.predict()`\n",
    "\n",
    "which you have already encountered in Task 2 of Worksheet 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24345913-6c35-4b6a-bdd3-add894041b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arange predictor matrix, adding transformations of 'x' up to order 9\n",
    "X=np.vstack((x10**0,x10,x10**2,x10**3,x10**4,x10**5,x10**6,x10**7,x10**8,x10**9))\n",
    "X=X.T\n",
    "# create inputs for a smooth function later on\n",
    "X1000=np.vstack((x1000**0,x1000,x1000**2,x1000**3,x1000**4,x1000**5,x1000**6,x1000**7,x1000**8,x1000**9))\n",
    "X1000=X1000.T\n",
    "# the sine wave we fit is still the same.\n",
    "Y=sine_wave\n",
    "reg = Ridge(alpha=0.00,fit_intercept=True, random_state=42)\n",
    "reg.fit(X,Y)\n",
    "y_pred = reg.predict(X1000)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(x10,sine_wave,s=50,edgecolor='blue',facecolor='w',marker='o')\n",
    "plt.plot(x1000,y_pred,color='r',label='$M$ = 9')\n",
    "plt.plot(x1000,sine_wave_smooth,color=lightgreen,label='True function')\n",
    "plt.legend(fontsize=16,loc='upper right')\n",
    "plt.title('$N$ = 10',size=20)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.xlabel('x',size=20)\n",
    "plt.ylabel('y',size=20)\n",
    "print('The mean squared error of your prediction on 1000 values of x is: ' +str(round(mean_squared_error(y_pred,sine_wave_smooth),2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ea188-0f54-40cd-bac1-0de587525f18",
   "metadata": {},
   "source": [
    "**Task 1.2** In the `Ridge()` regression object above, increase the value of $\\alpha$ to various positive values. What do you observe? Does the fit generally get better than for the unregularized case?\n",
    "\n",
    "*Hint*: also try out very small positive values for $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ff9e5-2708-4f36-8854-731a6c9a2eb9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3505feba557bbbbabd66a11a70d6d9d",
     "grade": true,
     "grade_id": "tune_alpha_manually",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Task 1.2\n",
    "answer_tuning_alpha_manually = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e2857-b702-4aaf-bdc2-8f106785e4f4",
   "metadata": {},
   "source": [
    "#### Cross-validation of the $\\alpha$ hyperparameter\n",
    "\n",
    "Now it's time to fix the poor generalization performance of the 9th-order polynomial in a more systematic fashion.\n",
    "\n",
    "To achieve this, choose one of two options to systematically vary the L2-regularization parameter $\\alpha$ across the following list of values:\n",
    "\n",
    "[1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1e+00, 1e+01, 1e+02, 1e+03, 1e+04, 1e+05, 1e+06]\n",
    "\n",
    "The two options are:\n",
    "\n",
    "1) Combine `sklearn`'s `Ridge()` function with a `GridSearchCV()` object. You can find examples for how to do this in its documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), just scroll to the bottom of the page.\n",
    "2) More directly use the `RidgeCV()` function from `sklearn`, which is a ridge regression object with built-in cross-validation, see its documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html).\n",
    "\n",
    "Either way, use a strategy called [leave-one-out cross-validation](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms). Note that for 10 samples this is equivalent to a 10-fold cross-validation strategy. For convenience, we have already imported both `GridSearchCV()` and `RidgeCV()` for you at the top of this worksheet. \n",
    "\n",
    "**Task 1.3** Carry out the cross-validation using at least one of the two approaches suggested. Once you have trained and cross-validated your function on the 10 samples in `X` and `Y`, use this function to make predictons on the `X1000` input defined above and:\n",
    "1) Estimate the `mean_squared_error()` of these predictions relative to `sine_wave_smooth`, and assign its value to `best_mse_cv_polynomial`.\n",
    "2) Pass the best $\\alpha$ value according to your cross-validation to the variable `best_alpha_polynomial` for evaluation after upload to Ilias. You might want to use `RidgeCV()` and `GridSearchCV()` attributes such as `.best_estimator_`, `.alpha_`, or `.alpha` depending on the path you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a9628-dcbb-47db-9307-5c220cdb2e7b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea470214ffd9d4e324fd73a1939bb0fe",
     "grade": true,
     "grade_id": "code_2D_reg_two_options",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Task 1.3\n",
    "best_mse_cv_polynomial = None\n",
    "best_alpha_polynomial = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa499d7-c42d-44d9-b628-224f47a5bfe5",
   "metadata": {},
   "source": [
    "**Task 1.4** Use `matplotlib` to visualize the 10 samples, the true sine wave function, and your new cross-validated 9th-order polynomial predictions on `X1000`. \n",
    "\n",
    "You are allowed to re-use code from above. Add sensible labels and make clear in the title that this is a fit on 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8afa4-a172-42ba-b747-999880238555",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cddc736a1ca234419677e34d8f881cf6",
     "grade": true,
     "grade_id": "visualization_ridgecv",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Task 1.4 - write your own matplotlib code\n",
    "plt.figure(figsize=(8, 4))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d8b80-039d-42d3-a343-d93a5d4f864c",
   "metadata": {},
   "source": [
    "## Part 2 - Global warming revisited\n",
    "\n",
    "- Last week you modelled historical global warming as a function of concentrations in atmospheric carbon dioxide (CO$_2$) and of various modes of climate variability. Back then we measured climate variability through a set of standard \"indices\".\n",
    "- Here, you will take a high-dimensional data science approach to this problem. Instead of the small set of indices, you will use global sea level pressure fields to characterize the internal dynamical state of Earth's atmosphere, on top of the trend driven by CO$_2$.\n",
    "\n",
    "For this, we will download [sea level pressure (SLP)](https://glossarytest.ametsoc.net/wiki/Sea_level_pressure) data from the [NCEP-NCAR-reanalysis-I](https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html). This data represents an estimate of monthly-mean fluctuations in SLP from the year January 1948 to October 2024, at 2.5$\\degree$ latitude $\\times$ 2.5$\\degree$ longitude spatial resolution.\n",
    "\n",
    "SLP is hardly influenced by trends in surface temperature. Instead, it can be seen as an almost pure representation of the internal dynamical variability of Earth's atmosphere, which makes it a useful \"orthogonal\" variable to CO$_2$.\n",
    "\n",
    "The next few (admittedly tedious) cells are required to fetch and load the monthly regional SLP data, the monthly NASA global mean temperature data (cf. Worksheet 1) and the monthly CO$_2$ data (cf. Worksheet 1, upsampled from annual mean data). \n",
    "\n",
    "We make sure to end up with array definitions that span a consistent time period from January 1948 to August 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6548a-2c2a-4902-8cf8-b4adf503c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch and process sea level pressure data\n",
    "# already done for you, we have moved it into the subfolder \"data\"\n",
    "# slp_file = netCDF4.Dataset(MLclimate.fetch_data('https://downloads.psl.noaa.gov/Datasets/ncep.reanalysis/Monthlies/surface/slp.mon.mean.nc','slp.mon.mean.nc'))\n",
    "# read out the variable, time dimension, and longitude-latitude coordinates, for background info see also Worksheet 1\n",
    "# constrain this data in time for consistency with the temperature data later on\n",
    "slp_file = netCDF4.Dataset('./data/slp.mon.mean.nc')\n",
    "nr_months = 920\n",
    "slp_lon = slp_file['lon'][:]\n",
    "slp_lat = slp_file['lat'][:]\n",
    "slp_mm_1948_2024 = slp_file['slp'][:nr_months,:,:]\n",
    "slp_time = netCDF4.num2date(slp_file['time'][:nr_months],slp_file['time'].units)\n",
    "# check time interval\n",
    "print(slp_time[0],slp_time[nr_months-1])\n",
    "# dimensions are: time, lat, lon\n",
    "print(slp_mm_1948_2024.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98c13f-a4dc-4325-8bc6-2e19aac02377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's load once again the monthly-mean global warming data from Worksheet 1.\n",
    "# Then we select the same time interval as for the SLP data\n",
    "# All that might take a minute...\n",
    "idx_nasa_1948 = (1948-1880)*12\n",
    "nasa_mm_file = netCDF4.Dataset('./data/gistemp1200_GHCNv4_ERSSTv5_fldmean.nc')\n",
    "### On Google Colab: comment out the line above and instead uncomment and execute the following:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload() \n",
    "### in the pop-up window \"Choose Files\" the gistemp_monthly_mean.nc file from the /data/ directory in your exercise folder\n",
    "# nasa_mm_file = netCDF4.Dataset('gistemp_monthly_mean.nc')\n",
    "nasa_mm_1948_2024 = nasa_mm_file['tempanomaly'][idx_nasa_1948:,0,0]\n",
    "nasa_time = netCDF4.num2date(nasa_mm_file['time'][idx_nasa_1948:],nasa_mm_file['time'].units)\n",
    "print(nasa_time[0],nasa_time[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195b2a7-9e94-478d-acc3-326680f763d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we load the CO2 data and constrain it to January 1948 - August 2024\n",
    "# again, we have prepared this for you and have put this into the folder \"data\"\n",
    "# co2_am_file_1850_2014 = MLclimate.fetch_data(\"https://files.isimip.org/ISIMIP3b/InputData/climate/atmosphere_composition/co2/historical/co2_historical_annual_1850_2014.txt\",\"co2_historical_annual_1850_2014.txt\")\n",
    "co2_am_1850_2014 = pd.read_csv('./data/co2_historical_annual_1850_2014.txt',header=None,delim_whitespace=True,names=['Year','CO2 (ppmv)'])\n",
    "# co2_am_file_2015_2024 = MLclimate.fetch_data(\"https://files.isimip.org/ISIMIP3b/InputData/climate/atmosphere_composition/co2/ssp126/co2_ssp126_annual_2015_2100.txt\",\"co2_ssp126_annual_2015_2100.txt\")\n",
    "co2_am_2015_2024 = pd.read_csv('./data/co2_ssp126_annual_2015_2100.txt',header=None,delim_whitespace=True,names=['Year','CO2 (ppmv)'])\n",
    "co2_am_1948_2024 = pd.concat([co2_am_1850_2014[co2_am_1850_2014['Year'] >= 1948],co2_am_2015_2024[co2_am_2015_2024['Year'] < 2026]]).reset_index(drop=True)\n",
    "co2_am_1948_2024.Year = pd.to_datetime(co2_am_1948_2024.Year,format='%Y')\n",
    "co2_am_1948_2024 = co2_am_1948_2024.to_xarray()\n",
    "co2_am_1948_2024 = co2_am_1948_2024.set_coords('Year').swap_dims({'index': 'Year'})\n",
    "### the CO2 data is only available at yearly temporal resolution, so we upsample the data according to the nearest timestamp to monthly frequency\n",
    "co2_mm_1948_2024 = co2_am_1948_2024.resample(Year='1M').nearest().to_dataframe()['CO2 (ppmv)'][:-5]\n",
    "co2_mm_1948_2024 = np.array(co2_mm_1948_2024.values)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5425a4-c355-4f60-b5d9-dd0004bdc7d3",
   "metadata": {},
   "source": [
    "**Task 2.1** To illustrate the approximate climate-invariance of the sea level pressure data stored in `slp_mm_1948_2024[:,:,:]`, create four subplots of SLP histories at four randomly chosen grid locations, i.e. show the location-dependent evolution of monthly SLP over time. You may re-use code from the beginning of Task 2 of Worksheet 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a2bb7-201f-44df-ab31-d33552b92fc5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2615f6a4f073f105b1057c75216375c",
     "grade": true,
     "grade_id": "time_series_slp",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.1\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77161419-a081-4dd0-8715-913bfa66281a",
   "metadata": {},
   "source": [
    "#### Cartopy for surface maps\n",
    "\n",
    "An alternative way to visualize Earth system data are surface maps, which can for example be created with the Python package `cartopy`. You can find many examples for geospatial data mapping in its [documentation](https://scitools.org.uk/cartopy/docs/latest/gallery/index.html).\n",
    "\n",
    "To provide one example here, let's visualize the SLP map for September 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91339972-fa29-4155-a4b1-a71bd18c5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example cartopy plot, see if you can derive the meaning of each point\n",
    "ax = plt.axes(projection=cartopy.crs.PlateCarree(central_longitude=0))\n",
    "ax.set_global()\n",
    "ax.gridlines(linestyle='--',color='gray')\n",
    "clevs = np.arange(980,1041,1)\n",
    "slp_cyclic, slp_lon_cyclic = add_cyclic_point(slp_mm_1948_2024,coord=slp_lon)\n",
    "plt.contourf(slp_lon_cyclic,slp_lat,slp_cyclic[-1],clevs,cmap='RdBu_r',extend='both',transform = cartopy.crs.PlateCarree())\n",
    "colorbar = plt.colorbar(ax=ax,orientation='horizontal',pad=0.1,aspect=20,shrink=0.5,ticks=np.array([980,1000,1020,1040]))\n",
    "colorbar.set_label(\"hPa\",size=12,rotation=0)\n",
    "colorbar.ax.tick_params(labelsize = 10)\n",
    "ax.set_xticks([-180,-120,-60,0, 60, 120, 180], crs=cartopy.crs.PlateCarree())\n",
    "ax.set_yticks([-90, -60, -30, 0, 30, 60, 90], crs=cartopy.crs.PlateCarree())\n",
    "lon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "lat_formatter = LatitudeFormatter()\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "ax.coastlines()\n",
    "plt.title('Sea level pressure August 2024',size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c30df-5a45-4181-84a6-ccde95637e64",
   "metadata": {},
   "source": [
    "**Task 2.2** Use the `Linear_Regression()` and `Ridge()` model objects from the `sklearn` package to `.fit()` and `.predict()` NASA global mean temperatures (`nasa_mm_1948_2024`) from the historical global mean CO$_2$ (`co2_mm_1948_2024`) and regional SLP (`slp_mm_1948_2024`) evolution; the latter across all 2.5$\\degree\\times$2.5$\\degree$ grid boxes globally. Owing to the large number of SLP predictors, this will result in a high-dimensional regression problem and in an underdetermined problem (number samples < number predictors) which can only be approximated numerically, which happens automatically when you use the `sklearn` functions.\n",
    "\n",
    "As always, you may re-use code from Worksheet 1 where applicable.\n",
    "\n",
    "Proceed in the following steps:\n",
    "1. Combine the predictor data (CO$_2$, SLP) into one large matrix `X` of dimensions (number_timesteps, number_predictors)\n",
    "2. Split the predictor and predictand data into training and test datasets: `X_train` and `y_train` and `X_test` and `y_test`. Use the period January 2000 to December 2009 for testing. Use all other months for training (and cross-validation, for ridge). See Worksheet 1 how you can for example use `np.delete()` to un/select the corresponding indices.\n",
    "3. Fit the multiple linear regression to the training data.\n",
    "4. Use the `StandardScaler()` object from `sklearn.preprocessing`, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), to scale all predictors to zero mean and unit standard deviation according to the samples in the training data. Then apply this transformation to both training and test data, to define new matrices `X_train_scaled` and `X_test_scaled`.\n",
    "5. Train the ridge regression model in combination with a `GridSearchCV()` object using 5-fold cross-validation, using `X_train_scaled` for the predictors. For the list of regularization parameters $\\alpha$ used in cross-validation, run multiple iterations in which you narrow down on an increasingly fine-grained range around the best $\\alpha$ selected. Repeat this until the final results are not particularly sensitive anymore to further finetuning of $\\alpha$. Make sure to set the `random_state` variable in `Ridge()` to the value 101 for reproducibility.\n",
    "6. Use both functions to predict the period January 2000 to December 2009.\n",
    "7. `.print()` the mean squared error for both methods over this test period.\n",
    "8. Assign these values to the matchingly named variables `mse_mlr_global` and `mse_ridge_global` for autograding.\n",
    "9. Save the most optimized `alpha` value of your `.best_estimator_` for the ridge regression to the variable `best_alpha_global`. Don't worry about hitting the `alpha` value precisely, a range of values will be accepted as close enough.\n",
    "\n",
    "**Task 2.3** Using `matplotlib`, visualize the three time series (true NASA, MLR predicted, and ridge predicted) for the test period January 2000 to December 2009. Make sure to add sensible labels etc. In addition, calculate the corresponding [Pearson correlation coefficients](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html) and [r2-scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) for both regression methods and add this info to the figure title. In the title, remember to convert numbers to `str()` and you can use `round()` to avoid printing too many decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dea155-f19f-4797-9dec-9ee99e20ffef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b5f525e438b745c9b063e2bfe3c51f8",
     "grade": true,
     "grade_id": "Implement_MLR_ridge_global",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.2, step-by-step\n",
    "mse_mlr_global = None\n",
    "mse_ridge_global = None\n",
    "best_alpha_global = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941fc3a-5acb-4e56-b774-dea7478c9d15",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca8e5e7cda1fa0e0a0096e44396a386a",
     "grade": true,
     "grade_id": "visualization_global_predictions",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.3\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc18b0-48c2-4e9f-811b-d35a7c01da85",
   "metadata": {},
   "source": [
    "**Task 2.4** Which of the two linear models performs better? Can you explain this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7bced-bdc8-4594-ace3-884397be37fb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c3d208a22eca075f43605108c23aa8c",
     "grade": true,
     "grade_id": "answer_curse",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.4\n",
    "answer_curse = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2d70f-968d-4ee9-9e22-a276bf9a060d",
   "metadata": {},
   "source": [
    "**Task 2.5** Can you explain why we needed to scale the predictors prior to carrying out the ridge regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63d239-3944-488e-b8fc-d219fa6f3656",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49ba70d47d642d0ee72ebd86047c5a58",
     "grade": true,
     "grade_id": "answer_scaling",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.5\n",
    "answer_scaling = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6d846-34fc-40e7-b8cf-f364280d3cc0",
   "metadata": {},
   "source": [
    "## Part 3 - Regional warming in the Karlsruhe area\n",
    "\n",
    "Above and in Worksheet 1, you modelled historical changes in **global mean** surface temperature using various factors of internal variability as well as changes in atmospheric CO$_2$ concentrations.\n",
    "\n",
    "In this part, we will look at how the global warming signal translates into regional surface warming. For this, we download and process a higher level NASA dataset, which also includes latitude-longitude resolution of historical Earth surface temperatures since 1880, even though we will again constrain ourselves to the period 1948 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8008383-df65-4587-9a83-a9e75112dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fetch NASA GISTEMP temperature data\n",
    "### we have done this for you and put the file in the \"data\" folder again\n",
    "# data_url_regional = \"https://data.giss.nasa.gov/pub/gistemp/gistemp1200_GHCNv4_ERSSTv5.nc.gz\"\n",
    "# data_file_regional = \"gistemp250_GHCNv4.nc.gz\"\n",
    "# data_file_regional = MLclimate.fetch_data(data_url_regional,data_file_regional)\n",
    "nasa_regional_nc = MLclimate.open_netcdf('./data/gistemp1200_GHCNv4_ERSSTv5.nc')\n",
    "print(nasa_regional_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bfafc-415e-4bb6-8c7f-5f33e50d89f3",
   "metadata": {},
   "source": [
    "#### Data extraction and exploration\n",
    "\n",
    "As implied by the last output, we now have a readable netCDF dataset, in the CLASSIC netCDF4 format but also compatible with the former NETCDF3_CLASSIC format.\n",
    "\n",
    "The file info contains e.g. a title, the publisher (NASA Goddard Institute for Space Studies), and the dimensionality of the gridded temperature data: there are 90 latitude coordinates, 180 for longitude, and 1736 timesteps.\n",
    "\n",
    "Let's explore this dataset further. First, we will list the variables and then read out the actual latitude, longitude coordinates, the time dimension, and the actual temperature data on the latitude-longitude grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4e578-df72-4d8e-874d-6f88a9d0020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_nasa = nasa_regional_nc['lat'][:]\n",
    "lon_nasa = nasa_regional_nc['lon'][:]\n",
    "time_nasa = nasa_regional_nc['time'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f8a3f-1e04-4f53-a91e-f829265da8d6",
   "metadata": {},
   "source": [
    "#### Latitude and longitude\n",
    "The latitude data runs from -89.0 to 89.0, in 2$\\degree$ latitude resolution. By convention negative numbers stand for Southern Hemisphere grid locations. Positive numbers stand for latitudes in the Northern Hemisphere. 0$\\degree$ latitude marks the equator.\n",
    "\n",
    "The longitude spatial resolution is also 2$\\degree$ and runs from -179$\\degree$ to 179$\\degree$. \n",
    "In this case, 0$\\degree$ is defined as running along the [null meridian in Greenwich, London, UK](https://www.britannica.com/place/Greenwich-meridian). Negative numbers move towards the west of that location, positive towards the east.\n",
    "At $\\pm$180$\\degree$ the two directions meet, because the Earth is a sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbabb6f-259e-47b9-bc4d-5e8dc6dfda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Latitudes\\n\", lat_nasa,\"\\n\")\n",
    "print(\"Longitudes\\n\", lon_nasa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554ca16-efaa-4692-84d8-b045a1f8474b",
   "metadata": {},
   "source": [
    "More info on each variable can be extracted with `.variable['VAR_NAME']`.\n",
    "\n",
    "For example, this concerns the units, which for longitude is \"degrees east\" (i.e. negative longitudes are located towards the west)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0defb-6376-4e2f-af34-8376debb1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_regional_nc.variables['lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343555a-7c19-4c45-a38b-e32233843545",
   "metadata": {},
   "source": [
    "#### Time\n",
    "As in Worksheet 1, we can handle the at first confusing time dimension with its range of values [29233, 81...] using standard `netCDF4` functions. The numbers actually represent the number of \"days since 1800-01-01\".\n",
    "\n",
    "This is a common issue with climate data, especially considering that this actually is monthly averaged data! Therefore, it is always good to know in advance which time range and temporal averaging you expect for a given dataset. As in Worksheet 1, you can use the `netCDF4.num2date` function to convert the time axis into Gregorian calendar format, which is more human-readable. This reveals that the monthly format of this data, starting in January 1880, with the timestamp centred on the 15th day of that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b011eb4-6fe4-4eec-b866-c5e58dd6e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracted values for dimension time:\\n\",time_nasa,\"\\n\")\n",
    "print(\"Info on the variable time:\\n\",nasa_regional_nc.variables['time'],'\\n')\n",
    "time_gregorian = netCDF4.num2date(time_nasa,units=nasa_regional_nc.variables['time'].units)\n",
    "print(\"Time dimension converted into Gregorian calendar:\\n\",time_gregorian[:1])\n",
    "print(\"Time dimension converted into Gregorian calendar:\\n\",time_gregorian[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231ec67-a48c-4608-8f9c-36dff15c67f0",
   "metadata": {},
   "source": [
    "#### Loading the predictand data \n",
    "Let's now load the monthly-mean surface temperature anomalies provided in the file, but starting from 1948. The anomalies are defined relative to the 1951-1980 period average, for each grid point.\n",
    "\n",
    "The dimensions of this variable are (number timesteps, number latitudes, number longitudes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e5f75-5fe8-494f-a946-cf357d86338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to make sure the last month is August 2024, even if the NASA website gets updated.\n",
    "nasa_regional_mm = nasa_regional_nc.variables['tempanomaly'][(1948-1880)*12:1736,:,:]\n",
    "print(nasa_regional_mm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49614625-7ab1-441a-8262-13cfc9e6413e",
   "metadata": {},
   "source": [
    "**Task 3.1** Using a `cartopy` surface map plot like the one shown above for SLP, visualize the average surface temperature difference for all grid locations between the latest approx. 20 years (2004 to August 2024) and the earliest 20 years of the time series (1948 to 1968). For averaging you might find the `numpy` function `.mean(,axis=)` helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84821605-d8fb-4919-b58e-6aa32e66c76c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e305a0ed7cd0f6f3458d78d81c428033",
     "grade": true,
     "grade_id": "ST_visualization",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.1\n",
    "ST_diff = np.zeros((len(lat_nasa),len(lon_nasa)))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb9ea9-b573-475b-b347-84c16c4d194b",
   "metadata": {},
   "source": [
    "**Task 3.2** Now repeat the procedure for Tasks 2.2 and 2.3 (visualization) above, but for a different regression set-up. As before, you are asked to fit a ridge regression to the temperature data, using regional SLP (all grid points, globally) and global CO$_2$ evolution as predictors. Use the period January 2000 to December 2009 as test data, and all other months for training and cross-validation.\n",
    "\n",
    "However, this time\n",
    "\n",
    "a) select as predictand time series the temperature evolution (1948 onwards) for **the grid cell that contains the city of Karlsruhe.** For this, you will need to find the corresponding location on the grid on which the NASA temperatures are provided. Note that Karlsruhe is [located](https://latitude.to/map/de/germany/cities/karlsruhe) at approx. 49$\\degree$N and 8$\\degree$ E. Since the grid resolution for the NASA grid is coarse and the data centred in boxes of 2$\\degree\\times 2\\degree$ spatial resolution, simply select the lat-lon coordinate closest to Karlsruhe so that we will consider temperature changes within a larger area that includes Karlsruhe. If Karlsruhe falls exactly between two coordinates, take the lower index number for simplicity.\n",
    "\n",
    "b) in addition, instead of MLR, this time fit [LASSO regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) next to ridge regression. As for ridge regression, cross-validate/tune its regularization parameter `alpha`. Add your code to the same cell below!\n",
    "\n",
    "Make sure you scale the predictors both for ridge and LASSO regression. Set the `random_state` in both regression objects to 222.\n",
    "\n",
    "Finally, assign the mean squared errors and best $\\alpha$ values to the logically named variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8987d1f-6578-4bff-a9b4-3fec965a5d00",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5f639af936cc086ae4743614ddd150e",
     "grade": true,
     "grade_id": "regressions_KA",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# implement regressions of Task 3.2\n",
    "mse_lasso_KA = None\n",
    "mse_ridge_KA = None\n",
    "best_alpha_lasso_KA = None\n",
    "best_alpha_ridge_KA = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc63eda-62d8-4d5c-8db8-775202982285",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5166919f4a3aa8afe336bee610f1233",
     "grade": true,
     "grade_id": "vis_KA",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create visualization of time series, this time for LASSO and Ridge\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea9672-b143-4909-bf74-f290b8c11070",
   "metadata": {},
   "source": [
    "**TASK 3.3** Could you suggest ways to improve the performance of machine learning regression models of this kind on test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7889e7-edd4-40ce-8dab-512a36384415",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e43b3fac556996b356ee6d26572cb616",
     "grade": true,
     "grade_id": "improve_predictions",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "answer_improve_predictions = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbea55-a94a-4e81-8d87-0c1359ae0b75",
   "metadata": {},
   "source": [
    "**Task 3.4** Finally, let's consider the interpretability of the ridge and LASSO models.\n",
    "\n",
    "Use `cartopy` to visualize the coefficients of the respective `GridSearchCV().best_estimator_` regression objects using their attribute `.coef_`, but only for the SLP predictors. This means that you need to exclude the coefficient/slope learned for the CO$_2$-related first predictor.\n",
    "\n",
    "You will then further need to `.reshape()` the dimensions of the resulting coefficient array into a lat-lon matrix of the spatial shape (number latitudes, number longitudes) of the original SLP data. Note that these dimensions are different from the NASA grid for regional temperature.\n",
    "\n",
    "**Task 3.5** Once you have visualized the coefficients: what is the key difference in the coefficient maps? Can you explain this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf8fbc-4367-401c-a4e7-c217ff5b6091",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daa7762c7e5b658b4b2840a71ac0c465",
     "grade": true,
     "grade_id": "vis_coeffs",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.4\n",
    "# visualize the coefficient maps for ridge and LASSO\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b26f2-de74-4601-a40b-56ab66cc74e3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80e3fe91479dfa0bbfb2369483ea3fa7",
     "grade": true,
     "grade_id": "explain_diff_ridge_lasso",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer Task 3.5\n",
    "answer_diff_lasso_ridge = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dee17f-4b43-4cc9-9eb5-3a89217ab3c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f94db30684e1960f9196cbb6c2cf6228",
     "grade": true,
     "grade_id": "manual_tuning_answer",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(answer_tuning_alpha_manually) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7655d7-f8e6-407b-a477-ab76130c8b70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f0474c79b31a55889ff52b74db4cfd8",
     "grade": true,
     "grade_id": "assert_best_alpha_2D_reg",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert best_alpha_polynomial is not None\n",
    "assert best_mse_cv_polynomial is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754058de-ac02-4139-8da5-b1dcd5a87a04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b77af9dd9b142f14ca23b4447abf16cc",
     "grade": true,
     "grade_id": "evaluate_global_warming_fits",
     "locked": true,
     "points": 5.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert mse_mlr_global is not None\n",
    "assert mse_ridge_global is not None\n",
    "assert best_alpha_global is not None\n",
    "assert len(answer_scaling) > 0\n",
    "assert len(answer_curse) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ceb56c-e24e-49ea-8113-1bbea3b62793",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b0d3470e0d4da986565f464091d581c",
     "grade": true,
     "grade_id": "ST_diff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A few hidden tests on ST_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380e04e-23f9-4d24-81dd-03f4558bbfea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23a20916f88cc776032c5cbc541eca6b",
     "grade": true,
     "grade_id": "KA_tests",
     "locked": true,
     "points": 5.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert mse_ridge_KA is not None\n",
    "assert mse_lasso_KA is not None\n",
    "assert best_alpha_ridge_KA is not None\n",
    "assert best_alpha_lasso_KA is not None\n",
    "assert len(answer_improve_predictions) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c0f47-361f-421c-b80c-d2ea4446ac25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29f72e320b7285c9d36ab95bb2bdcb13",
     "grade": true,
     "grade_id": "answer_ridge",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(answer_diff_lasso_ridge) > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-climate-2024",
   "language": "python",
   "name": "ml-climate-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
